{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Credentials"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "!mkdir ~/.aws"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/cyprien/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/.aws/credentials\n",
    "\n",
    "[default]\n",
    "aws_access_key_id=\n",
    "aws_secret_access_key="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/cyprien/.aws/config\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/.aws/config\n",
    "\n",
    "[default]\n",
    "region=eu-west-3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export to s3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(12654, 12654)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the directory you want to search\n",
    "folder_path = '../../basalt_neurips_data/MineRLBasaltMakeWaterfall-v0/'\n",
    "\n",
    "# Set the file extensions you want to search for\n",
    "extensions = [\"_preprocessed.mp4\", \"_preprocessed.jsonl\", \"_preprocessed_annotations.jsonl\"]\n",
    "\n",
    "files = []\n",
    "\n",
    "# Iterate through the files in the directory\n",
    "for file in os.listdir(folder_path):\n",
    "    # Check if the file name ends with any of the specified extensions\n",
    "    if any(file.endswith(ext) for ext in extensions):\n",
    "        # If the file name ends with any of the extensions, print the file name\n",
    "        files.append(file)\n",
    "\n",
    "len(files), len(set(files))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 3854/5154 [07:05<04:45,  4.56it/s]  "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.invalid_files import pp_video, pp_actions, pp_annotations\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "\n",
    "def export_files_to_s3(bucket_name, s3_prefix, local_folder, file_extensions):\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    files = []\n",
    "\n",
    "    for file_name in os.listdir(local_folder):\n",
    "        if any([file_name.endswith(file_extension) for file_extension in file_extensions]):\n",
    "            files.append(file_name)\n",
    "\n",
    "    # Create a ThreadPoolExecutor with a certain number of threads\n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "      # Iterate through all the files in the folder\n",
    "        for file_name in tqdm(files[7500:]):\n",
    "        # Check if the file has the desired extension\n",
    "            if any([file_name.endswith(ext) for ext in file_extensions]):\n",
    "              # Construct the full path to the file\n",
    "              file_path = os.path.join(folder_path, file_name)\n",
    "              # Construct the full S3 object key (including the path within the bucket)\n",
    "              s3_key = os.path.join(s3_prefix, file_name)\n",
    "              # Check if the file already exists in the S3 bucket\n",
    "            try:\n",
    "                s3_object = s3.head_object(Bucket=bucket_name, Key=s3_key)\n",
    "                # If the file exists, check if the size of the local file is the same as the size of the S3 object\n",
    "                if os.path.getsize(file_path) == s3_object['ContentLength']:\n",
    "                # If the sizes are the same, skip the file\n",
    "                    continue\n",
    "            except s3.exceptions.ClientError as e:\n",
    "              # If the file does not exist, or if there is any other error, catch the exception and proceed with the upload\n",
    "                pass # TODO\n",
    "\n",
    "            # Submit a task to the executor to upload the file to S3\n",
    "            executor.submit(s3.upload_file, file_path, bucket_name, s3_key)\n",
    "            # try:\n",
    "            #     future.result()\n",
    "            # except Exception as e:\n",
    "            #     print(f'Warning: failed to upload file {file_name} with function call export_files_to_s3({bucket_name}, {s3_prefix}, {local_folder}, {file_extensions})')\n",
    "\n",
    "\n",
    "\n",
    "bucket_name = 'basalt-neurips'\n",
    "folder_path = '../../basalt_neurips_data/MineRLBasaltMakeWaterfall-v0/'\n",
    "s3_prefix = 'basalt_neurips_data/MineRLBasaltMakeWaterfall-v0/'\n",
    "\n",
    "export_files_to_s3(bucket_name, s3_prefix, folder_path, [pp_video, pp_actions, pp_annotations])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import from s3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils.invalid_files import pp_video, pp_actions, pp_annotations\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def download_files_from_s3(bucket_name, s3_prefix, local_folder, file_extensions):\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    continuation_token = None\n",
    "    objects = []\n",
    "\n",
    "    # Loop until all objects have been retrieved\n",
    "    while True:\n",
    "        # List the objects in the specified S3 prefix\n",
    "        response = s3.list_objects(\n",
    "            Bucket=bucket_name, Prefix=s3_prefix, ContinuationToken=continuation_token\n",
    "        )\n",
    "        # Append the objects to the list\n",
    "        objects.extend(response['Contents'])\n",
    "        # Check if there are more objects to retrieve\n",
    "        if 'NextContinuationToken' in response:\n",
    "            # If there are more objects, set the continuation token and continue the loop\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            # If there are no more objects, break out of the loop\n",
    "            break\n",
    "\n",
    "    # Create a ThreadPoolExecutor with a certain number of threads\n",
    "    with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "        # Iterate through the objects\n",
    "        for obj in objects['Contents']:\n",
    "            # Get the object key (i.e., the file name)\n",
    "            key = obj['Key']\n",
    "            # Check if the file has the desired extension\n",
    "            if any([key.endswith(file_extension) for file_extension in file_extensions]):\n",
    "                # Construct the full path to the local file\n",
    "                local_file_path = os.path.join(local_folder, key)\n",
    "                # Check if the file already exists locally\n",
    "                try:\n",
    "                    local_file_size = os.path.getsize(local_file_path)\n",
    "                    # If the file exists, check if the size of the local file is the same as the size of the S3 object\n",
    "                    if local_file_size == obj['Size']:\n",
    "                        # If the sizes are the same, skip the file\n",
    "                        continue\n",
    "                except FileNotFoundError:\n",
    "                    # If the file does not exist, proceed with the download\n",
    "                    pass\n",
    "                # Submit a task to the executor to download the file from S3\n",
    "                future = executor.submit(s3.download_file, bucket_name, key, local_file_path)\n",
    "\n",
    "\n",
    "download_files_from_s3('basalt-neurips', 'basalt_neurips_data/MineRLBasaltMakeWaterfall-v0/', '../../', [pp_video, pp_actions, pp_annotations])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
